{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "\n",
    "def _load_label_names():\n",
    "    \"\"\"\n",
    "    Load the label names from file\n",
    "    \"\"\"\n",
    "    return ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "\n",
    "def load_cfar10_batch(cifar10_dataset_folder_path, batch_id):\n",
    "    \"\"\"\n",
    "    Load a batch of the dataset\n",
    "    \"\"\"\n",
    "    with open(cifar10_dataset_folder_path + '/data_batch_' + str(batch_id), mode='rb') as file:\n",
    "        batch = pickle.load(file, encoding='latin1')\n",
    "\n",
    "    features = batch['data'].reshape((len(batch['data']), 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "    labels = batch['labels']\n",
    "\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "def display_stats(cifar10_dataset_folder_path, batch_id, sample_id):\n",
    "    \"\"\"\n",
    "    Display Stats of the the dataset\n",
    "    \"\"\"\n",
    "    batch_ids = list(range(1, 6))\n",
    "\n",
    "    if batch_id not in batch_ids:\n",
    "        print('Batch Id out of Range. Possible Batch Ids: {}'.format(batch_ids))\n",
    "        return None\n",
    "\n",
    "    features, labels = load_cfar10_batch(cifar10_dataset_folder_path, batch_id)\n",
    "\n",
    "    if not (0 <= sample_id < len(features)):\n",
    "        print('{} samples in batch {}.  {} is out of range.'.format(len(features), batch_id, sample_id))\n",
    "        return None\n",
    "\n",
    "    print('\\nStats of batch {}:'.format(batch_id))\n",
    "    print('Samples: {}'.format(len(features)))\n",
    "    print('Label Counts: {}'.format(dict(zip(*np.unique(labels, return_counts=True)))))\n",
    "    print('First 20 Labels: {}'.format(labels[:20]))\n",
    "\n",
    "    sample_image = features[sample_id]\n",
    "    sample_label = labels[sample_id]\n",
    "    label_names = _load_label_names()\n",
    "\n",
    "    print('\\nExample of Image {}:'.format(sample_id))\n",
    "    print('Image - Min Value: {} Max Value: {}'.format(sample_image.min(), sample_image.max()))\n",
    "    print('Image - Shape: {}'.format(sample_image.shape))\n",
    "    print('Label - Label Id: {} Name: {}'.format(sample_label, label_names[sample_label]))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(sample_image)\n",
    "\n",
    "\n",
    "def _preprocess_and_save(normalize, one_hot_encode, features, labels, filename):\n",
    "    \"\"\"\n",
    "    Preprocess data and save it to file\n",
    "    \"\"\"\n",
    "    features = normalize(features)\n",
    "    labels = one_hot_encode(labels)\n",
    "\n",
    "    pickle.dump((features, labels), open(filename, 'wb'))\n",
    "\n",
    "\n",
    "def preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode):\n",
    "    \"\"\"\n",
    "    Preprocess Training and Validation Data\n",
    "    \"\"\"\n",
    "    n_batches = 5\n",
    "    valid_features = []\n",
    "    valid_labels = []\n",
    "\n",
    "    for batch_i in range(1, n_batches + 1):\n",
    "        features, labels = load_cfar10_batch(cifar10_dataset_folder_path, batch_i)\n",
    "        validation_count = int(len(features) * 0.1)\n",
    "\n",
    "        # Prprocess and save a batch of training data\n",
    "        _preprocess_and_save(\n",
    "            normalize,\n",
    "            one_hot_encode,\n",
    "            features[:-validation_count],\n",
    "            labels[:-validation_count],\n",
    "            'preprocess_batch_' + str(batch_i) + '.p')\n",
    "\n",
    "        # Use a portion of training batch for validation\n",
    "        valid_features.extend(features[-validation_count:])\n",
    "        valid_labels.extend(labels[-validation_count:])\n",
    "\n",
    "    # Preprocess and Save all validation data\n",
    "    _preprocess_and_save(\n",
    "        normalize,\n",
    "        one_hot_encode,\n",
    "        np.array(valid_features),\n",
    "        np.array(valid_labels),\n",
    "        'preprocess_validation.p')\n",
    "\n",
    "    with open(cifar10_dataset_folder_path + '/test_batch', mode='rb') as file:\n",
    "        batch = pickle.load(file, encoding='latin1')\n",
    "\n",
    "    # load the training data\n",
    "    test_features = batch['data'].reshape((len(batch['data']), 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "    test_labels = batch['labels']\n",
    "\n",
    "    # Preprocess and Save all training data\n",
    "    _preprocess_and_save(\n",
    "        normalize,\n",
    "        one_hot_encode,\n",
    "        np.array(test_features),\n",
    "        np.array(test_labels),\n",
    "        'preprocess_training.p')\n",
    "\n",
    "\n",
    "def batch_features_labels(features, labels, batch_size):\n",
    "    \"\"\"\n",
    "    Split features and labels into batches\n",
    "    \"\"\"\n",
    "    for start in range(0, len(features), batch_size):\n",
    "        end = min(start + batch_size, len(features))\n",
    "        yield features[start:end], labels[start:end]\n",
    "\n",
    "\n",
    "def load_preprocess_training_batch(batch_id, batch_size):\n",
    "    \"\"\"\n",
    "    Load the Preprocessed Training data and return them in batches of <batch_size> or less\n",
    "    \"\"\"\n",
    "    filename = 'preprocess_batch_' + str(batch_id) + '.p'\n",
    "    features, labels = pickle.load(open(filename, mode='rb'))\n",
    "\n",
    "    # Return the training data in batches of size <batch_size> or less\n",
    "    return batch_features_labels(features, labels, batch_size)\n",
    "\n",
    "\n",
    "def display_image_predictions(features, labels, predictions):\n",
    "    n_classes = 10\n",
    "    label_names = _load_label_names()\n",
    "    label_binarizer = LabelBinarizer()\n",
    "    label_binarizer.fit(range(n_classes))\n",
    "    label_ids = label_binarizer.inverse_transform(np.array(labels))\n",
    "\n",
    "    fig, axies = plt.subplots(nrows=4, ncols=2)\n",
    "    fig.tight_layout()\n",
    "    fig.suptitle('Softmax Predictions', fontsize=20, y=1.1)\n",
    "\n",
    "    n_predictions = 3\n",
    "    margin = 0.05\n",
    "    ind = np.arange(n_predictions)\n",
    "    width = (1. - 2. * margin) / n_predictions\n",
    "\n",
    "    for image_i, (feature, label_id, pred_indicies, pred_values) in enumerate(zip(features, label_ids, predictions.indices, predictions.values)):\n",
    "        pred_names = [label_names[pred_i] for pred_i in pred_indicies]\n",
    "        correct_name = label_names[label_id]\n",
    "\n",
    "        axies[image_i][0].imshow(feature*255)\n",
    "        axies[image_i][0].set_title(correct_name)\n",
    "        axies[image_i][0].set_axis_off()\n",
    "\n",
    "        axies[image_i][1].barh(ind + margin, pred_values[::-1], width)\n",
    "        axies[image_i][1].set_yticks(ind + margin)\n",
    "        axies[image_i][1].set_yticklabels(pred_names[::-1])\n",
    "        axies[image_i][1].set_xticks([0, 0.5, 1.0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 3:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 994, 1: 1042, 2: 965, 3: 997, 4: 990, 5: 1029, 6: 978, 7: 1015, 8: 961, 9: 1029}\n",
      "First 20 Labels: [8, 5, 0, 6, 9, 2, 8, 3, 6, 2, 7, 4, 6, 9, 0, 0, 7, 3, 7, 2]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 9 Max Value: 255\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 2 Name: bird\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfcAAAHxCAYAAABwLPU6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAWJQAAFiUBSVIk8AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAGgZJREFUeJzt3cuPpfl5F/Dfe251Tt373j33ydjENgYMFkGwCGKBWJFEgISULFkgseBvQeIvQGIDLBDZgJBgkUCcYBHbcXwZj8f2XLp7erq6uru6qutc38MiK4xYPM8MNeTR57P/6jlz6pz322cz32673TYAoI7BF/0CAIDPl3IHgGKUOwAUo9wBoBjlDgDFKHcAKEa5A0Axyh0AilHuAFCMcgeAYpQ7ABSj3AGgGOUOAMUodwAoZvRFv4D/h2zZ8n+VXjruriSStl6tUrnttg9nFotl6tZ6E7/VWmuDLv5O7u3NUreGo3EqV1VmGvzxyUnq1k/efTeVOz09DWe2/SZ1a7WJ50bD1Kn2W7/1j1KPEL/cAaAY5Q4AxSh3AChGuQNAMcodAIpR7gBQjHIHgGKUOwAUo9wBoBjlDgDFKHcAKEa5A0Axyh0Aiqm8CpeSWT/ic5JaXMttriUGxlprufW0y8t56taDB4/CmR/+4L3UrafPX4QzL87PU7cWy9wSV+YD8trr91KX/ubf+EY48+q9W6lbg2FuLuwqn1XbPnErk8nearmlx+SpzzAreXX8cgeAYpQ7ABSj3AGgGOUOAMUodwAoRrkDQDHKHQCKUe4AUIxyB4BilDsAFKPcAaAY5Q4AxRiO+Rx02RWSK3SVIxPZ92O5WIQzDx48TN26f//TVO7nH8Tvffjh/dStx49Pw5lPnzxL3Tqfr8KZ7XqZunV8fJzKbQaTcOZb330/desP/+dPwpm/9de/lrr1d379r6VyN25cC2fyj4FEMHss+fzoBvEBnuyTe5AYkLrqlvDLHQCKUe4AUIxyB4BilDsAFKPcAaAY5Q4AxSh3AChGuQNAMcodAIpR7gBQjHIHgGKUOwAUo9wBoBircH/OXOW6W2u5hbeHDx6lbn3rD74dzvz+7/1R6tb7H+SW2har+BrUepVbT7tx/SicGU2mqVvDYXxxre/j70Vrrb14/jyVu+zjn8XRcJy69f0fPQ1nPr7/SerWD37081Tud/7x3wtnvvTOG6lbfeK50yeW01rLr0pmculbqdTVPrv9cgeAYpQ7ABSj3AGgGOUOAMUodwAoRrkDQDHKHQCKUe4AUIxyB4BilDsAFKPcAaAY5Q4AxRiO+XMnNz5wdnaeyn37238Szvzu7/7n1K2HD+PDG9tumLo1GMeHUlprbTZcx0M7ude4M52FM/NFbqRms4n/dw263G+DwSA3u3Hx7Ek4s7u7n7o1Gccfjc8Tr6+11v77H+WGdObzRTjzT//JP0jdeuvNe+FMcpMlOcrSWrvKUa3USI3hGADgM1DuAFCMcgeAYpQ7ABSj3AGgGOUOAMUodwAoRrkDQDHKHQCKUe4AUIxyB4BilDsAFKPcAaAYq3Cfg77vU7nMOtb9j+6nbv2rf/0fUrnvff+9cGb58kXq1nYYX0/bOzpM3drtcn+z+UV8Xa9PjkEtl/HVr8vLeepWN4g/CrbJVbidae6xMx3Hvy/zxWXq1v7sdjizXV2kbrV17jX+8Xfii43/4l/mvpv//J/9djhz7Xgvdasl19OudnMtrsvv3aX45Q4AxSh3AChGuQNAMcodAIpR7gBQjHIHgGKUOwAUo9wBoBjlDgDFKHcAKEa5A0Axyh0AilHuAFCMVbjPQdfl1n6ePn0azvyX//qt1K3vfuf7qdzOYXwda3pwLXVr/vIsnNl2uY/wOrngNUgs1w2S62n9ehnOjCeT1K3L5SacGXXr1K3lMpcbJt7GwWaVuvX4wS/CmdF4lro1GY9Tuek4/oZ88OGD1K1/82//Yzjzm3//11O3hoP4d6y11rbb+C5cIpKW7Yksv9wBoBjlDgDFKHcAKEa5A0Axyh0AilHuAFCMcgeAYpQ7ABSj3AGgGOUOAMUodwAoRrkDQDGGYz4H2z4+utFaa48ePg5nfvSTX6RuXVwuUrmd3XiuH+QGNFpiWGG7s5c61W/7XG4dH7fZmU1Ttzbz+Gvshsn3fif+7/zNxZPUqcvscMwo/rgaLOPjO6211jbx3GBnJ3VqNMt9hkejxOpJcrzk9Fl85Or+x/dTt6Z7ue9La5n/ttxyTCa1vdrdGL/cAaAa5Q4AxSh3AChGuQNAMcodAIpR7gBQjHIHgGKUOwAUo9wBoBjlDgDFKHcAKEa5A0Axyh0AirEK90syo0mX89zi2scfPwhnRi23qHW0n1usOj/9OJzpRrmVq8Uq/j7uJled2jD3fnTTo3Bmscotk42Gk3CmHyX/u1bzeGg0Tt0aJXObVfyzv3frrdStozcPwpnh9DB1a5BcldycPwxnzp/Elyhba+3kJL7Udv9hbjXwlTvx71hrrS1fvghnhuPcZzG1YJl8VGX55Q4AxSh3AChGuQNAMcodAIpR7gBQjHIHgGKUOwAUo9wBoBjlDgDFKHcAKEa5A0Axyh0Aium2V/1/s786qf+wzPvRb3LDD2dnZ+HMYpEY+Git/Y//9vup3H/6d/8+nPnoeepUa4M+HJnM9lOn+nF8CKO11kY7s8Sx3Hdss4kPpfTD3BbUahm/1SVeX2utbZKf4cy7ONm7kbq1M40P8MwvX6ZuzZe54alucxkPLRKZ1trRcXzM5eAwN6Tz9beup3I3juLfzdn13K3hLH5rNEqskrXWfvM3/mEq6Jc7ABSj3AGgGOUOAMUodwAoRrkDQDHKHQCKUe4AUIxyB4BilDsAFKPcAaAY5Q4AxSh3AChGuQNAMbkJKf43w1HubbyWWCTq+/hyWmutXT++lsodJcbT7p/kZuG2+/EFr4Nbr6RuHRzGV65aa+3i2aNwphslluRaa/NNfAzq/OxZ6tb28jweWuXWzBbJ9bSui3/2B/MnqVvDceJ3T/L96Oa51cB1F1+uWyY/i/2L+N9slHwunj7PLTYeH+2FM+lR1ERu0OVW4bL8cgeAYpQ7ABSj3AGgGOUOAMUodwAoRrkDQDHKHQCKUe4AUIxyB4BilDsAFKPcAaAY5Q4AxSh3ACjGKtwXKDNItFktU7eW69wiUTfdD2f2x6e5W4fH4cy1O2+kbl2/llvJG9y+E8588PgsdevyNLFoNsitfk0G8SW/brhJ3RpOc5/FncTTand3krp1OIsvk43n69StJ+e5Nbm7k/jn6o8fv0jdGszuJVK5v/PLxSqVO38Zfx93j5KzcJlVuOT7keWXOwAUo9wBoBjlDgDFKHcAKEa5A0Axyh0AilHuAFCMcgeAYpQ7ABSj3AGgGOUOAMUodwAoxnDMFygzI3B6cpK69d5776Vyj9bx4ZjxtVdStw5uvxrObPrUqdbtHKRyD59ehjOfPE4MwLTW+tU8nDmc5f69Pu52w5nNPDfwMRrspHKHh/HXOBrHB2Baa222ig+s7J4/yt3ay73Gg7340NLhZW6k5vpr18OZ5UX889taa8v4V6y11lq/jb/G9To3fjTYJEaCuqutW7/cAaAY5Q4AxSh3AChGuQNAMcodAIpR7gBQjHIHgGKUOwAUo9wBoBjlDgDFKHcAKEa5A0Axyh0AirEK9wVarZbhzI9/+KPUre9+L5dbjeNLS7fevJu6tR3FV7+68TB1a93n5uQeffownNksz1O3Nuv452Peb1O39vfiS23j2WHq1mScfOxs4/9t62VumezkRWKRb5H7TE0HF6nc+6ujcGY5iWdaa216fCec6VcfpW5dPD9N5R5/Gn9+TCa5hcLr00QuMwP6GfjlDgDFKHcAKEa5A0Axyh0AilHuAFCMcgeAYpQ7ABSj3AGgGOUOAMUodwAoRrkDQDHKHQCKMRzzBRp08SWB4+Pj1K29vVkqN7+Ij57cuPVW6tb5Ip4Z7d5I3Xpy8iiVW83Pwpl+kxtzadt1OHI0y32lbxzvhzPr9TR1q203uVwfz83PciMkg0n8fXw2/oupW8+ev0jlXrb4d3qyl3t+jHbjgzNHN+PDR621Nu8TD4LW2qCLf8+6xDP4z27Fc9vkqFOWX+4AUIxyB4BilDsAFKPcAaAY5Q4AxSh3AChGuQNAMcodAIpR7gBQjHIHgGKUOwAUo9wBoBjlDgDFWIX7HGy3ubWf4Wgczty5dzd16/V7N1O5zaeJxapRbi2s6+Pvx+5RbuXqvZ/+OJWbz+OLVdNRn7p1bboTztw4GKZuXSzii2urVW71azrOvcadYTy3mcTXzFpr7fx8Hs4s+9xvpbNt7jXuJJbrDuIfqdZaaxePfhHO7E5yt46OD1O51956I5zZPcq9913is3jV/HIHgGKUOwAUo9wBoBjlDgDFKHcAKEa5A0Axyh0AilHuAFCMcgeAYpQ7ABSj3AGgGOUOAMUYjvkcdN3V3To43E/l7r7xdir3yfxROLNc50YVjm/eDme65GjP7k5u1WK1F3//D0cXqVu399bhzPllPNNaa6cv4yMw02HuvV+vcrmLLj7Ac7HJfTl3j+ODIvvb+PhOa60dJwd4FudPwpnLk9znY3t8LZy5dfd66tadO6+kckc3b4Uz093d1K1tH/9bd1f8U9ovdwAoRrkDQDHKHQCKUe4AUIxyB4BilDsAFKPcAaAY5Q4AxSh3AChGuQNAMcodAIpR7gBQjHIHgGKswn0OksNkqTW59Ta3uLaZ5ZaW5ttn4czOIPexOjo8Dmfe+/Gfpm5dnH6Syn34J38Qzrz26s3UrfHdV8OZk4vcClo/jP/NdnZyf+fROL7u1lpr3eoynDke5lbQbk/i7+PhIPfeP13l1uR+lsj1m/h72Fpre+P4Ktx+cnHt+u07qdxgFP88bvrcw7vrkg/9K+SXOwAUo9wBoBjlDgDFKHcAKEa5A0Axyh0AilHuAFCMcgeAYpQ7ABSj3AGgGOUOAMUodwAoRrkDQDFW4X5JZqltu82tQWU8OZvnche5Ja4vf+Vr4cy1G7dTtx6fnoUz2+0ydevF00ep3OnJSTw0yC35dcdfimf291O3jlr8vb8zzH0Wzz/JLfLNn8UXCm9Np6lb3WQWzjzc5h6nHy9yn+HzTXwV7ub1+PJia63dvRP/Tt+4lVx3G09yuVH8e7bdJhcKm1U4AOCKKXcAKEa5A0Axyh0AilHuAFCMcgeAYpQ7ABSj3AGgGOUOAMUodwAoRrkDQDHKHQCKMRzzf7i6EZiM8/PLVK7v4yMTrbV27258MOLJ80Xq1uXF83Dmxo0bqVuv/MrXU7npjdfCmfF0N3XrcCf+WZw8f5C6dfnhz8KZnzz8OHXreHqQyo228bGOD5dPU7cWB4fhzPhm7rM4muWeOfur+FDKfL5O3eoTvwNvJsZmWmutG++kctsu/hqzwzHbxGcxEflM/HIHgGKUOwAUo9wBoBjlDgDFKHcAKEa5A0Axyh0AilHuAFCMcgeAYpQ7ABSj3AGgGOUOAMUodwAoxirc5yI79xNfg+rW89Sl5cvcOtbPH8Q/Ih/fP0nd2h8vw5n5Mvd+XL+dW6waj+IrUtsnH6VuLd+LL7X97Kfvp249ehL/fAzHucfH7PXjVO55F182HN69lbr1+huvxENdbt3t5NmzVC5zbzyepU7dSXxfdnenqVuXueG6tu0T383kKlxrmVtXuzjqlzsAFKPcAaAY5Q4AxSh3AChGuQNAMcodAIpR7gBQjHIHgGKUOwAUo9wBoBjlDgDFKHcAKMZwzC/LbsBckVvX91K56TA3kPDuB/fDmX75InXr4dOPw5nF2aepW+OL3FjHw/d/HM4sPs0N6Wzm8aGU08VF6lbr4v/Onwxyj48nw1Ss3X3n9XBmNs0dO7+MDxKt+twwyKYbp3LdTvz9f+2tN1K33vmVt8OZo+PcQND89Hkq123iz7h+G/+O/Zl4brC92rr1yx0AilHuAFCMcgeAYpQ7ABSj3AGgGOUOAMUodwAoRrkDQDHKHQCKUe4AUIxyB4BilDsAFKPcAaAYq3C/JDMK13W5NajW4itGB3uz1KXDndxrXJz8NJzpz+Lrbq21dvLuT+K3TnKLa9fH01SuOzkPZ/bGuXWsg7vx3NuD3L/Xf3YaX9cbvXYjdetX/+qXU7l+GV9qWwwPUreGB0fhzLjLLS/e3N1J5YZd/Gn1zV/7ZurW2++8Fc68eJlcKNzm3sdULrkK1yWaIl0TSX65A0Axyh0AilHuAFCMcgeAYpQ7ABSj3AGgGOUOAMUodwAoRrkDQDHKHQCKUe4AUIxyB4BilDsAFGMV7pd0iaW2bZ9bMVov4itXz5+cpm69+2F89au11v70e98NZ84fvpu6tftiEc58Zf9m6ta1SXKp7c7tcGY0HqduTdownHk6Wqdu/epXXw1n7n3p9dSt9WAvldtOroczb7z2ZurWZhN/H/fGmU3J1l4+e5jKvfpK/G/2la99NXVrfzdeFS8uc6tw2+RS2zbz7E4u0HWJRb7Mktxn4Zc7ABSj3AGgGOUOAMUodwAoRrkDQDHKHQCKUe4AUIxyB4BilDsAFKPcAaAY5Q4AxSh3ACim7HDM4vxZKtdv4/9z/81qlbr18vw8nPn0JDccs+xmqdxiHf/332KT+zfj7nQ/nFkuU6faT08/SuVOL5+HMzvD3Nds9+Yr4cy9X/ty6taXvvmNcGb26l9O3epnubGf7Tr+Pbt2OEndunsnPlJz9iA3mHTaX6Zyx9dvhTOzWe45MBrFB1b65FBK3+fGjzaJDZjM87611rpEZpu8leWXOwAUo9wBoBjlDgDFKHcAKEa5A0Axyh0AilHuAFCMcgeAYpQ7ABSj3AGgGOUOAMUodwAoRrkDQDFlV+He+8H3UrnhaBzOZMd+lstFOPPJaTzTWmsvk+tpw8RS23Q/vqjVWmt9H99auhzmVq7m48SEVGvtchJfJlsf5d6Pr//dvx3O/IWvfy116+XwMJxZ9LnfBjtdbvVrvXgSz7wYpm5dTuOv8eSTh6lb+wdHqdzx9eNwZjzJreSdvzgJZ9bL3Fpmy30127ZPLNdtkg/vLrEempmt+wz8cgeAYpQ7ABSj3AGgGOUOAMUodwAoRrkDQDHKHQCKUe4AUIxyB4BilDsAFKPcAaAY5Q4AxZQdjnn3hz9I5Yaj+FsyGGTfxvj4wNl6mro0HOcGI26/9nY4s9jPvca2mocjw/FO6tT48m4qF59Xae2vfONLqVtf/UtfDmcef/I4devi2Qfx0M5e6tbOIDeg8fo78fdx2+e+mz/7zh+GM4eHufdjNMj9xhqOEyNXLT7O1Fprz56ehjP9KjccM93JPavW6004s2y519j38VtrwzEAwGeh3AGgGOUOAMUodwAoRrkDQDHKHQCKUe4AUIxyB4BilDsAFKPcAaAY5Q4AxSh3AChGuQNAMWVX4Z49e54LJkaTBl1uaSljM9xP5UbtOJWb7sbvTUevpG6NE2thZy/OU7dmh/FFvtZae+c4nttbfpy69ePf+2k4s1wuUre6YfxRsDPNbOS19mJxkcqtXz4LZ45u3UvdWs7jn6t7r7+aunXjbi53/eaNcGYyyf2eWy2X4cxy/jJ1a9zlvpttEM9th7lTm23iNW7XuWNJfrkDQDHKHQCKUe4AUIxyB4BilDsAFKPcAaAY5Q4AxSh3AChGuQNAMcodAIpR7gBQjHIHgGKUOwAUU3YVrnW5f7dsW3ztp89MybXWusy/rfpV6tZskFskunb9ejgzGeQW6JaXL8KZ/ckmdWu2fJjKtZMH4cjjl5epU8s+Plk1meVWA8eJ3GqRW+QbjnJTXP0yviY3v8y9xmuvvBHOTPZyn/uzi/jiWmutrVfxZ8HzeW41cH45D2fWq+RCYXJlc5BYXRu0+BJla621xHJdl1mS+wz8cgeAYpQ7ABSj3AGgGOUOAMUodwAoRrkDQDHKHQCKUe4AUIxyB4BilDsAFKPcAaAY5Q4AxZQdjhmMZqncNrNZkBw62Cb+bbXtc0MpN2e5cYrR5CicWS3iAx+ttbbdxEc+ti03iPPsLD6E0Vprm1X8b70d5j6L/To+vLFc5d6P4Sj+fmy3k9St269+OZUbjsbhTJ/8bu7vxv9mo/E0devg2q1UbjSIPz/ufxQfPmqttfUmMVjV5UZZuuTo1zDxp94mh2PWiYGxreEYAOCzUO4AUIxyB4BilDsAFKPcAaAY5Q4AxSh3AChGuQNAMcodAIpR7gBQjHIHgGKUOwAUo9wBoJiyq3Cb5BJXy6z9ZFfhEutHfXLFaDLKrcIdrB+GM+v+ZerWuosvk21me6lbL6bXUrn5PL7U1mXmqlprq8RncXWRXLvr469xdrCTutVvh6ncahH/DM+m+6lb09luOLOzG8+01tponHsMT3biz7g333o7dev9738SzqyXuc/iZJJbGxykno2552m/TaxzXu0onF/uAFCNcgeAYpQ7ABSj3AGgGOUOAMUodwAoRrkDQDHKHQCKUe4AUIxyB4BilDsAFKPcAaCYssMx/TA3PrDdxv/v/oMrXAToBrkRkuEo937s7sbv9ZPca7w8i48xXKxzH+HtJDc404/iuUnyn9DTFn8fp7m3vu0eHMdv7eVGWbKf4cFwHM4cHsb/u1prbTo7CGc269wIyc4g9wGZjOPvx2icG7fp+lU4s17lhmNaZpSltdYSz+51n3t2r1N/6+SXM8kvdwAoRrkDQDHKHQCKUe4AUIxyB4BilDsAFKPcAaAY5Q4AxSh3AChGuQNAMcodAIpR7gBQjHIHgGK6zAoaAPD/L7/cAaAY5Q4AxSh3AChGuQNAMcodAIpR7gBQjHIHgGKUOwAUo9wBoBjlDgDFKHcAKEa5A0Axyh0AilHuAFCMcgeAYpQ7ABSj3AGgGOUOAMUodwAoRrkDQDHKHQCKUe4AUIxyB4BilDsAFKPcAaAY5Q4AxSh3AChGuQNAMcodAIpR7gBQjHIHgGKUOwAUo9wBoBjlDgDFKHcAKEa5A0Axyh0AilHuAFCMcgeAYpQ7ABSj3AGgGOUOAMUodwAoRrkDQDHKHQCKUe4AUMz/AkWuNrnrQewfAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 248,
       "width": 251
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 3\n",
    "sample_id = 5\n",
    "display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    x = np.array(x)\n",
    "    x_max = np.max(x)\n",
    "    x_min = np.min(x)\n",
    "    output = (x - x_min)/(x_max - x_min)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(range(10))\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    \n",
    "    return lb.transform(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images = []\n",
    "all_labels = []\n",
    "for batch_id in range(1, 6):\n",
    "    filename = 'preprocess_batch_' + str(batch_id) + '.p'\n",
    "    features, labels = pickle.load(open(filename, mode='rb'))\n",
    "    \n",
    "    all_images.extend(features)\n",
    "    all_labels.extend(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'preprocess_validation.p'\n",
    "valid_features, valid_labels = pickle.load(open(filename, mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images = np.array(all_images)\n",
    "all_labels = np.array(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 32, 32, 3)\n",
      "(45000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(all_images.shape)\n",
    "print(all_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Dense, Flatten, MaxPool2D, Dropout\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(64, (4, 4), padding=\"same\",input_shape=all_images.shape[1:], activation='relu', data_format='channels_last'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(64, (4, 4), padding=\"same\",input_shape=all_images.shape[1:], activation='relu', data_format='channels_last'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(64, (4, 4), padding=\"same\",input_shape=all_images.shape[1:], activation='relu', data_format='channels_last'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 32, 32, 64)        3136      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 16, 16, 256)       147712    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1024)              16778240  \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 17,194,058\n",
      "Trainable params: 17,194,058\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer='adam',metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "45000/45000 [==============================] - 825s 18ms/step - loss: 1.6178 - acc: 0.4092\n",
      "Epoch 2/10\n",
      "45000/45000 [==============================] - 909s 20ms/step - loss: 1.2345 - acc: 0.5621\n",
      "Epoch 3/10\n",
      "45000/45000 [==============================] - 895s 20ms/step - loss: 1.0838 - acc: 0.6182\n",
      "Epoch 4/10\n",
      "45000/45000 [==============================] - 973s 22ms/step - loss: 0.9828 - acc: 0.6554\n",
      "Epoch 5/10\n",
      "45000/45000 [==============================] - 1027s 23ms/step - loss: 0.9003 - acc: 0.6837\n",
      "Epoch 6/10\n",
      "45000/45000 [==============================] - 989s 22ms/step - loss: 0.8454 - acc: 0.7030\n",
      "Epoch 7/10\n",
      "45000/45000 [==============================] - 1003s 22ms/step - loss: 0.7878 - acc: 0.7226\n",
      "Epoch 8/10\n",
      "45000/45000 [==============================] - 883s 20ms/step - loss: 0.7322 - acc: 0.7420\n",
      "Epoch 9/10\n",
      "45000/45000 [==============================] - 958s 21ms/step - loss: 0.7011 - acc: 0.7517\n",
      "Epoch 10/10\n",
      "45000/45000 [==============================] - 941s 21ms/step - loss: 0.6537 - acc: 0.7681\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xe7f8fd0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(all_images, all_labels, epochs=10, batch_size=56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 20s 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7764536685943604, 0.7326]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(valid_features, valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_pred = model.predict(valid_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[354,   4,  14,   6,  19,   3,   8,   2,  58,  21],\n",
       "       [  9, 454,   0,   3,   3,   0,  10,   1,  17,  37],\n",
       "       [ 27,   2, 262,  25,  85,  11,  50,  10,  11,   2],\n",
       "       [ 16,   2,  24, 266,  52,  60,  58,  16,  10,   6],\n",
       "       [  6,   0,   6,  15, 378,   7,  26,  27,   7,   1],\n",
       "       [  7,   1,  21, 103,  38, 228,  35,  34,   2,   2],\n",
       "       [  0,   5,  13,  17,  27,   2, 431,   2,   0,   2],\n",
       "       [  5,   0,   8,  19,  46,  20,   2, 394,   1,   4],\n",
       "       [ 15,  11,   1,   5,   7,   0,   7,   2, 471,   6],\n",
       "       [  9,  31,   1,   9,   7,   2,   6,   6,  19, 425]], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(valid_labels.argmax(axis=1), valid_pred.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Pre-Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from IPython.display import display\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from PIL import Image\n",
    "sys.modules['Image'] = Image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\tensorflows\\lib\\site-packages\\PIL\\Image.py\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "print(Image.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\tensorflows\\lib\\site-packages\\PIL\\Image.py\n"
     ]
    }
   ],
   "source": [
    "import Image\n",
    "print(Image.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet50(weights='imagenet')\n",
    "\n",
    "img_path = 'tiger.jpeg'\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "    \n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "preds = model.predict(x)\n",
    "# decode the results into a list of tuples (class, description, probability)\n",
    "# (one such list for each sample in the batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\n",
      "40960/35363 [==================================] - 0s 6us/step\n",
      "Predicted: [('n02129604', 'tiger', 0.82760346), ('n02123159', 'tiger_cat', 0.16761719), ('n02127052', 'lynx', 0.002437188)]\n"
     ]
    }
   ],
   "source": [
    "print('Predicted:', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
